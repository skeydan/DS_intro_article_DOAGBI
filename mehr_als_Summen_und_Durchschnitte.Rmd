---
title: "Mehrals"
output: html_document
---

```{r setup, echo=FALSE}
library(ggplot2)
library(gridExtra)
library(dplyr)
library(datasauRus)
library(ggthemes)
library(ggExtra)
data("algae", package="DMwR2")

```


## Daten

Daten sind gut. Sie liefern uns Informationen, wieviele Einheiten wir verkauft haben; wie unser Umsatz war; wie es mit der Zufriedenheit unserer Kunden aussieht. Welche Werbebanner am meisten geklickt wurden. Welche unserer Strategien am besten funktioniert hat. Woraus wir unsererseits folgern können, wie wir jetzt am besten weiter vorgehen. Daten, konvertiert in Erkenntnisgewinn - und finanziellen Gewinn.

Daten sind schlecht. Daten sind "zuviel", um mit dem menschlichen Auge erfasst werden zu können. Wir müssen sie irgendwie zusammenfassen, vereinfachen. Wieviel Vereinfachung ist ausreichend? Einen Schritt zurück: Was überhaupt ist eine sinnvolle Vereinfachung, ein angemessenes "summary" unserer Daten? Wie aussagekräftig ist der Mittelwert, den ich in den meisten Reports angezeigt bekomme?

Daten sind Vergangenheit, _in the past_. Vergangene Aktienkurse; vergangene Verkaufszahlen; vergangene Adclicks. Interessiert mich der Schnee von gestern? Ja - weil ich auf den Schnee von morgen schliessen möchte! Wie kann ich das machen? Wie kann ich die in den Daten enthaltene Information optimal nutzen und die relevanten Variablen "herausfischen"? Hier ist das Business-Know-How wichtig - aber nicht alles. Methoden aus Data Science, Machine Learning und Statistik warten nur darauf, mich zu unterstützen.

Und schliesslich gibt es eine Klasse von Daten, die anders sind - scheinbar unstrukturiert; vielschichtig; extrem hochauflösend und überwältigend schon aufgrund ihrer Quantität. Für uns Menschen ist es selbstverständlich, dass wir Sprache verstehen, einen Hund von einer Katze unterscheiden können, dass fahrende Autos anders klingen, wenn es nass als wenn es trocken ist. Auf Pixelebene aber gibt es nichts, was einen Hund zum Hund oder eine Katze zur Katze macht. 
Diese Daten, diese Aufgabenstellungen sind das Reich von Deep Learning, das mit gar nicht so neuen Konzepten, aber immer ausgefeilteren Algorithmen und viel Rechenpower jeden Monat neue Rekorde zu brechen scheint. Aber das betrifft doch nur "die Grossen" - Google, Facebook, Microsoft, Apple ... oder? Oder ist Deep Learning auch etwas für mich und meine Daten?

## Pfade durch den Jungle

Wenn heute Daten ein Jungle sind, sind Algorithmen es noch mehr. In diesem Artikel wollen wir Pfade durch den Algorithmenjungle ziehen, die Ihnen helfen sollen, die für Ihre Anforderungen und Bedürfnisse relevanten Methoden zu finden.
Daten werden dabei für uns vor allem in zwei Inkarnationen auftreten: Zum einen als Herausforderung. Hier geht es darum, die übergrosse Datenmenge in den Griff zu bekommen, zu summarisieren, ohne wichtige Features zu übersehen oder zu verfälschen.
Zum anderen als Chance. Welche Methoden stellen Data Science, Machine Learning, Deep Learning uns zur Verfügung, um Muster in den Daten zu finden und valide Schlussfolgerungen zu ziehen? Dabei geht es nicht um ein Potpourri der Methoden, ein Who's who der Algorithmen mit dem Ziel, alles einmal erwähnt zu haben. Es geht um den Überblick: Wie unterscheiden sich die Herangehensweisen, was sind die Konsequenzen, wenn ich mich für Methode B statt Methode A entscheide? Welches Vorgehen passt zu welchen Daten, zu welchen Fragestellungen? Es geht darum, den Jungle zu "mappen", um das geeignete Vorgehen für den konkreten Anwendungsfall finden zu können.

# Teil 1: Daten als Herausforderung

Bevor wir fragen, was Data Science und Artificial Intelligence uns geben können, sollten wir schauen, was wir mitnehmen auf die Reise durch den Dschungel. Das ist v.a. ein Verständnis für die Grundlagen - wie exploriere ich Daten, wie charakterisiere ich sie so präzis wie möglich?


## Daten als Herausforderung: Was ist ein gutes Summary?

Dass Mittelwerte ein fragwürdiges Summary sein können, hat jeder schon einmal gehört. Aber warum eigentlich ist das so bzw. in welchen Fällen?
Schauen wir uns zunächst einen Fall an, wo es nicht schadet, einen Mittelwert anzugeben: Abb. 1a.

```{r, echo=FALSE}
g1 <- ggplot(algae, aes(x = mxPH)) + geom_density(na.rm = TRUE) + 
  geom_vline(aes(xintercept = mean(algae$mxPH, na.rm = TRUE), linetype = "Mittelwert"), color = 'red') + 
  geom_vline(aes(xintercept = median(algae$mxPH, na.rm = TRUE), linetype = "Median"), color = 'green') + 
  scale_linetype_manual(name = "Mass", values = c(2, 2), 
                      guide = guide_legend(override.aes = list(color = c("green", "red")))) +
 xlab("") + ylab("Dichte") + theme(legend.position="bottom")

g2 <- ggplot(algae, aes(x = a1)) + geom_density(na.rm = TRUE) + 
  geom_vline(aes(xintercept = mean(algae$a1, na.rm = TRUE), linetype = "Mittelwert"), color = 'red') + 
  geom_vline(aes(xintercept = median(algae$a1, na.rm = TRUE), linetype = "Median"), color = 'green') + 
  scale_linetype_manual(name = "Mass", values = c(2, 2), 
  guide = guide_legend(override.aes = list(color = c("green", "red")))) +
  xlab("") + ylab("Dichte") + theme(legend.position="bottom") + ggtitle("Dataset A")

grid.arrange(g1, g2, ncol = 2)
```

Der Mittelwert liegt tatsächlich mittig in der Verteilung, direkt neben dem Median, dem Wert, unterhalb und oberhalb dessen jeweils die Hälfte der Werte liegen. Tatsächlich ist die hier gemessene Variable mehr oder weniger normalverteilt. Eine Normalverteilung ist symmetrisch und durch Mittelwert und Streuung (Varianz) hinreichend charakterisiert.


### Der Median

Wie kann es aber ausschauen, wenn die Daten weniger "schön normalverteilt" sind? Ein Beispiel sehen wir in Abb. 1b. Während der Median das Gros der Daten, das nahe 0 liegt, gut beschreibt, ist der Mittelwert weit nach rechts verschoben. Warum? Beim Median kommt es allein auf die Ordnung der Werte an, die Abstände sind unerheblich. Beim Mittelwert gehen die Abstände direkt in die Berechnung ein. Krasses Beispiel: Der Median der Liste (1,2,3,4,1000) ist 3, der Mittelwert aber 202!

Insofern ist der Median, wenn es um knappe Charakterisierung der Daten geht, schon einmal ein besserer Kandidat als der Mittelwert. Für wirklich wichtige Daten und wenn ich den Platz habe, ersetzt aber nichts die Darstellung der gesamten Verteilung. 

Wie aussagekräftig ist z.B. der Median, wenn ich Daten habe wie in Abb. 2?

```{r, echo=FALSE, warning=FALSE, message=FALSE}
betas <- rbeta(100,.5,.5)
ggplot(data.frame(x=betas), aes(x = x)) + geom_histogram(bins = 20) +
  geom_vline(aes(xintercept = mean(x), linetype = "Mittelwert"), color = 'red') + 
  geom_vline(aes(xintercept = median(x), linetype = "Median"), color = 'green') + 
  scale_linetype_manual(name = "Mass", values = c(2, 2), 
                      guide = guide_legend(override.aes = list(color = c("red", "green")))) +
  ylab("Anzahl") + ggtitle("Dataset B")

  
```


### Boxplots

Eine beliebte klassische Methode, eine ganze Verteilung zu visualisieren, statt sie auf eine einzige Zahl zu reduzieren, ist - seit langem - das Boxplot (Abb. TBD). Für den Fall in Abb. TBD funktioniert das auch ziemlich gut: Wir sehen zum einen den Median (dicke horizontale Linie) und die für den Namen verantwortliche Box. In der Box liegen alle Werte zwischen dem 25. und dem 75. Perzentil. (Kurz zur Terminologie: Das n-te Perzentil, oder auch 0.n Quantil, ist der Wert, unterhalb dessen in der Verteilung n Prozent der Werte liegen. Der Median ist also auch das 50. Perzentil oder 0.5 Quantil. Häufig trifft man auch auf die Bezeichnung Quartil; das 1. Quartil ist dann gleich dem 25. Perzentil, das 3 . Quartil gleich dem 75. Perzentil.) Die vertikalen Linien ("Barthaare") haben mit dem sog. inter quartile range (IQR) zu tun. Der IQR ist der Abstand zwischen dem 1. und dem 3. Quartil. Das obere Barthaar reicht dann bis zu dem grössten Wert, der nicht mehr als 1.5 mal IQR von der oberen Boxgrenze entfernt ist; das untere entsprechend bis zum kleinsten Wert, dessen Entfernung zur unteren Boxgrenze nicht mehr als 1.5 mal IQR beträgt. Werte, die dann noch ausserhalb liegen, sind als Punkte eingezeichnet und werden oft als Outlier gesehen.
Was zeigt uns nun das Boxplot in Abb. TBD? Man sieht unmittelbar, dass das obere Rechteck viel höher ist als das untere: Das heisst, zwischen dem 25. Perzentil und dem Median liegen die Daten sehr eng zusammen; zwischem dem Median und dem 75. Perzentil liegen sie weiter gestreut. Dasselbe Bild zeigen uns die "Barthaare": die obere Linie reicht sehr weit nach oben, während die untere direkt bei 0 trunkiert wird. Nicht zufällig sind, was hier dargestellt wird, dieselben Daten wie in Abb. TBD. Wir können die Aussagekraft des Boxplots noch steigern, indem wir, wie in Abb. TBD geschehen, Mittelwert und Modus (den häufigsten Wert der Verteilung, in diesem Fall 0) hinzufügen.


```{r, echo=FALSE}
g1 <- ggplot(algae, aes(x = 1, y = a1)) + geom_boxplot() + ylab("")
mod <-algae %>% group_by(a1) %>% summarize(n = n()) %>% arrange(desc(n)) %>% select(a1) %>% head(1)
g2 <- ggplot(algae, aes(x = 1, y = a1))  + geom_boxplot() + 
  geom_hline(aes(yintercept = mean(algae$a1)), color = "green", linetype=2) + 
  geom_hline(aes(yintercept = mod), color = "blue", linetype=4)  + 
  ylab("")
grid.arrange(g1, g2, ncol= 2, top = "Dataset A")
```

### Violin plots

Wie hilfreich ist ein Boxplot für die Daten in abb. TBD? Abb. TBD zeigt es: Man sieht nicht ohne weiteres, dass hier eine bimodale Verteilung, mit den Modi an den beiden Enden, vorliegt. Das TBD an der y -Achse, um das wir das Boxplot ergänzt haben, ist da schon hilfreicher. Warum dann aber nicht gleich die ganze Verteilung zeigen, wenn das genauso viel Platz braucht? Abb. TBD zeigt ein sog. violin plot, nichts anderes als eine rotierte und gespiegelte Dichtekurve. Hier sieht man sofort, was los ist!



```{r, echo=FALSE}
g1 <- ggplot(data_frame(y = betas), aes(x = 1, y = betas)) + geom_boxplot() + geom_rug() +
  ylab("")
g2 <- ggplot(data_frame(y = betas), aes(x = 1, y = betas)) + geom_violin() + geom_rug() + ylab("")

grid.arrange(g1, g2, ncol= 2, top = "Dataset B")
```

### Quantile plots

Auf den ersten Blick weniger leicht zu lesen als violin plots, dafür aber sehr aussagekräftig sind quantile plots. Hier werden die Originaldaten dem Wert nach geordnet und dann gegen ihre Position in der Ordnung, ausgedrückt als Proportion zwischen 0 und 1, geplottet.

```{r, echo=FALSE}
sorted <- sort(algae$a1)
df <- data_frame(x = (1:(length(sorted)) - 0.5)/length(sorted), y = sorted)
g1 <- ggplot(df, aes(x=x, y=y)) + geom_point() + scale_x_continuous(c(0.0,1.0)) +
  ggtitle("Dataset A")
sorted <- sort(betas)
df <- data_frame(x = (1:(length(sorted)) - 0.5)/length(sorted), y = sorted)
g2 <- ggplot(df, aes(x=x, y=y)) + geom_point() + scale_x_continuous(c(0.0,1.0)) +
  ggtitle("Dataset B")
grid.arrange(g1, g2, ncol= 2)
```

Von den Quantilplots können wir nicht nur problemlos die wichtigen Quantile (0.25, 0.5, 0.75) ablesen.
Wir haben zudem durch die Steigung ein gutes Gefühl für die Dichte der Daten. Vergleichen wir die beiden Quantilplots in Abb. TBD und Abb. TBD: Links haben wir Dataset A, bei dem die niedrigen Werte eng zusammenliegen. Die sehr flache Steigung links des Medians zeigt, dass die Hälfte der Daten zusammenkommt, ohne dass der Wert merklich steigt. Nach dem Median nimmt die Kurve zunehmend Fahrt auf: Jetzt werden mit hinzukommenden Daten immer grössere Wertabstände überwunden.
Ganz anders Dataset B, das nahezu eine S-Kurve beschreibt. Die Steigung ist maximal in der Gegend um den Median herum. An beiden Enden liegen mehr Werte als in der Mitte, daher die flache Steigung an den Enden des S.

### Informationsdichte und Plot-Design

Womit wir uns hier beschäftigt haben, ist die Aussagekraft und Informationsdichte von Plots und Kennzahlen. Das Wort Informationsdichte kann man aber auch wörtlicher verstehen: Wie lassen sich Plots designen, die den vorhandenen Raum optimal ausnutzen, d.h. bei gegebener räumlicher Beschränkung maximale Information übermitteln?


Leider haben wir hier nicht den Raum, ausführlich auf gestalterische Aspekte der Visualisierung einzugehen. Wer sich hierfür interessiert, dem seien die Bücher von Edward Tufte empfohlen [1]. Ein guter Einstieg ist der Essay über den Cognitive Style of Powerpoint([2]). Wer den zum ersten Mal liest, wird nicht nur seine ggf. präexistenten Bedenken gegen den bullet point style bestätigt finden (von Gewährspersonen wie Richard Feynman!): Das Eindrucksvollste ist, wie absurd die geringe Informationsdichte von typischen Powerpoint-"Tabellen" oder - Diagrammen plötzlich erscheint, wenn man sie mit hoch informativen Grafiken wie sparklines([3]) oder slope graphs([4]) vergleicht.


## Daten als Herausforderung: Warum ist die Streuung der Daten wichtig?

Zurück zu den Kennzahlen. Bisher haben wir nur über Summaries bestehender Daten gesprochen.
Vielleicht ist das auch genug: Vielleicht möchte ich tatsächlich nur zusammenfassen, wie, unter gegebenen, ganz konkreten Bedingungen, "die Zahlen sich entwickelt haben". Aber oft will ich mehr: in andere Kontexte extrapolieren, zukünftige Werte vorhersagen oder (worum es im zweiten Teil gehen wird!) Zusammenhänge zwischen Variablen finden.

Sobald wir verallgemeinern, vorhersagen oder extrapolieren, verlassen wir das Reich der Gewissheiten. Es ist essentiell, dass wir uns über den Unsicherheitsfaktor im klaren sind und ihn quantifizieren können. Die meisten der Methoden, die wir in Teil zwei besprechen werden, kommen bereits mit Massen für die Unsicherheit. Was kann man machen, wenn die Methode kein solches Mass liefert? 

Eine universell einsetzbare Methode ist der Bootstrap. Das geht so: Ich möchte die Beziehung zwischen zwei Variablen, sagen wir x und y, "allgemeingültig" charakterisieren, habe aber nur ein fixes Set von Daten. Dieses Dataset ist eine mögliche Stichprobe von Werten aus der Population aller Werte. Nun behandle ich einfach mein Dataset, meine Stichprobe, selber als Population: Ich ziehe wiederholt Stichproben aus der Population und berechne jeweils die Beziehung zwischen x und y. (Die Analogie ist cum grano salis zu nehmen; beim Bootstrap handelt es sich um _sampling mit replacement_, d.h. ein und derselbe Wert darf mehrmals gezogen werden, während das "in der Realität" meist nicht der Fall sein dürfte.) Jetzt habe ich also z.B. 100 Schätzungen meines Kennwertes und berechne die Standardabweichung, die durchschnittliche quadrierte Abweichung der Schätzungen von ihrem Mittelwert.

Gut - zum Umgang mit Streuung und Unsicherheit ist also der wichtigste Punkt, dass wir sie angeben müssen.
Aber was exakt soll ich angeben? Vielleicht haben Sie schon einmal die Floskel "+/-2 Standardabweichungen" gehört. Woher kommt das und was bedeutet diese Information?
Grundlage des ganzen ist das Central Limit Theorem. Das Central Limit Theorem besagt, dass Mittelwerte einer Zufallsvariable im Limit unendlicher Stichprobengrösse normalverteilt sind. Die Normalverteilung der Mittelwerte hat als Mittelwert den wahren Mittelwert und als Standardabweichung den sog. Standardfehler des Mittelwerts (die Standardabweichung der gemessenen Variable, geteilt durch die Wurzel der Stichprobengrösse). 
Was bringt uns das? Für die Normalverteilung wissen wir, wieviel Prozent der Verteilung zwischen dem Mittelwert und einer, zwei oder drei Standardabweichungen liegen (Abb. TBD)

```{r, echo=FALSE}
normal_prob_area_plot <- function(lb, ub, mean = 0, sd = 1, limits = c(mean - 4 * sd, mean + 4 * sd), fcolor = "blue", title = "") {
    x <- seq(limits[1], limits[2], length.out = 100)
    xmin <- max(lb, limits[1])
    xmax <- min(ub, limits[2])
    areax <- seq(xmin, xmax, length.out = 100)
    area <- data.frame(x = areax, ymin = 0, ymax = dnorm(areax, mean = mean, sd = sd))
    (ggplot()
     + geom_line(data.frame(x = x, y = dnorm(x, mean = mean, sd = sd)),
                 mapping = aes(x = x, y = y))
     + geom_ribbon(data = area, mapping = aes(x = x, ymin = ymin, ymax = ymax), fill = fcolor)
     + scale_x_continuous(limits = limits) + ggtitle(title))
}
g1 <- normal_prob_area_plot(-3,3, title = "99% Dichte")
g2 <- normal_prob_area_plot(-2,2, title = "95% Dichte")
g3 <- normal_prob_area_plot(-1,1, title = "68% Dichte")
grid.arrange(g3,g2,g1, ncol=2, nrow=3, top="")
```

Damit kann ich für meine Parameterschätzung ein sog. Konfidenzintervall angeben. Leicht vereinfachend ausgedrückt: Wenn ich sage, meine Schätzung für den Parameter ist p +/- 2 mal Standardfehler, dann bin ich mir 95% sicher, dass der Parameter in diesem Bereich liegt. (Das ist in der Tat eine Vereinfachung. Siehe Literatur (TBD).) 
Was ich jetzt noch bedenken muss: Ein Konfidenzintervall für einen Parameter ist nicht dasselbe wie ein sog. Vorhersageintervall (_prediction interval_). Das Vorhersageintervall bezieht sich auf eine konkrete Vorhersage für neue Daten, und ist (z.T. erheblich) breiter als das Konfidenzintervall. Wir werden in Teil 2 ein Beispiel sehen.


## Daten als Herausforderung: Was charakterisiert eine Beziehung?

"Die Korrelation zwischen Investition in Onlinemarketing und Produktabsatz beträgt 0.8". Ein hervorragendes Ergebnis, oder? (Der Korrelationskoeffizient liegt zwischen 0 und 1; tatsächlich ist 0.8 eine sehr hohe Korrelation.) Nun ja, vielleicht. Auch hier gilt wieder: Ein Bild sagt mehr als 1000 Kennzahlen.
Nehmen wir an, ich habe ein bivariates Dataset mit Variablen x und y. Ich kenne den Mittelwert von x (~54.26), den Mittelwert von y (~47.83) und die jeweiligen Standardabweichungen (~16.76 resp. ~26.93). Vor allem kenne ich die Korrelation zwischen x und y (~-0.06). r = -0.06 bedeutet, die Daten sind unkorreliert, es gibt also keinen interessanten Zusammenhang. Wirklich?
Schauen wir auf Abb. TBD: Alle diese Datasets erfüllen die o.g. Bedingungen ([TBD]). Jede Menge Zusammenhänge - nur eben nicht der lineare, den der Korrelationskoeffizient voraussetzt!

```{r, echo=FALSE}
ggplot(datasaurus_dozen, aes(x=x, y=y, colour=dataset))+
    geom_point()+
    theme_void()+
    theme(legend.position = "none")+
    facet_wrap(~dataset, ncol=3)
```

Berühmt ist auch Simpson's Paradox, bei dem sich die Einschätzung komplett ins Gegenteil verkehrt, je nachdem, ob man die Daten als Gesamtheit betrachtet oder getrennt nach Subgruppen. Siehe Abb. TBD: Die bei Gesamtbetrachtung stark positive Beziehung der Variablen wird zu einer stark negativen, wenn man das Dataset in Gruppen aufteilt.

```{r, echo=FALSE}
ggplot(simpsons_paradox, aes(x=x, y=y, colour=dataset))+
    geom_point()+
    theme_void()+
    theme(legend.position = "none")+
    facet_wrap(~dataset, ncol=3)
```

Wie visualisiere ich nun am besten den Zusammenhang zwischen zwei Variablen? Das klassische Scatterplot ist dazu hervorragend geeignet. Beim normalen Scatterplot ist es allerdings nicht unbedingt einfach, die univariaten Verteilungen zu "sehen". Hier hilft es, z.B. ein Histogramm oder eine Dichtekurve hinzuzufügen, parallel zu den Achsen (Abb. TBD)

```{r, echo=FALSE}
g1 <- ggplot(datasaurus_dozen_wide, aes(x = bullseye_x, y = bullseye_y)) + geom_point() +
  theme_tufte(ticks=F) + theme(axis.title=element_blank(), axis.text=element_blank())
g1 <- g1 %>% ggMarginal(type = "density")
g2 <- ggplot(datasaurus_dozen_wide, aes(x = bullseye_x, y = bullseye_y)) + geom_point() +
  theme_tufte(ticks=F) + theme(axis.title=element_blank(), axis.text=element_blank())
g2 <- g2 %>% ggMarginal(type = "histogram", fill="transparent", bins = 20)
grid.arrange(g2,g1, ncol=2)
```

Diese Form der Visualisierung funktioniert hervorragend für bivariate Daten. Was, wenn die Zahl der interessierenden Varialen weit höher ist als 2?

## Daten als Herausforderung: Aber die Dimensionalität meiner Daten ist viel zu hoch!

dataset: dmwr / mnist
https://www.analyticsvidhya.com/blog/2017/01/t-sne-implementation-r-python/

```{r, echo=FALSE}
iris_unique <- unique(iris) # Remove duplicates
iris_matrix <- as.matrix(iris_unique[,1:4])
set.seed(42) # Set a seed if you want reproducible results
tsne_out <- Rtsne(iris_matrix) # Run TSNE

# Show the objects in the 2D tsne representation
plot(tsne_out$Y,col=iris_unique$Species)

# Using a dist object
tsne_out <- Rtsne(dist(iris_matrix))
plot(tsne_out$Y,col=iris_unique$Species)
```

## Daten als Chance: 

## 

## Literatur
Visualisierung: Chambers, Tufte 
https://www.inf.ed.ac.uk/teaching/courses/pi/2016_2017/phil/tufte-powerpoint.pdf

http://charliepark.org/slopegraphs/

https://www.edwardtufte.com/bboard/q-and-a-fetch-msg?msg_id=0001OR&topic_id=1


New Statistics, Lakens


https://github.com/stephlocke/datasauRus/
https://www.autodeskresearch.com/publications/samestats

