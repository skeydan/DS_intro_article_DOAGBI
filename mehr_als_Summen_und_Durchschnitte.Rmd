---
title: "Mehrals"
output: html_document
---

```{r setup, echo=FALSE}
library(ggplot2)
library(gridExtra)
library(dplyr)
library(datasauRus)
library(ggthemes)
library(ggExtra)
library(ggfortify)
library(forecast)
library(readr)
library(GGally)
data("algae", package="DMwR2")
data("AirPassengers")
data("Default", package = "ISLR")

```


## Daten

Daten sind gut. Sie liefern uns Informationen, wieviele Einheiten wir verkauft haben; wie unser Umsatz war; wie es mit der Zufriedenheit unserer Kunden aussieht. Welche Werbebanner am meisten geklickt wurden. Welche unserer Strategien am besten funktioniert hat. Woraus wir unsererseits folgern können, wie wir jetzt am besten weiter vorgehen. Daten, konvertiert in Erkenntnisgewinn - und finanziellen Gewinn.

Daten sind schlecht. Daten sind "zuviel", um mit dem menschlichen Auge erfasst werden zu können. Wir müssen sie irgendwie zusammenfassen, vereinfachen. Wieviel Vereinfachung ist ausreichend? Einen Schritt zurück: Was überhaupt ist eine sinnvolle Vereinfachung, ein angemessenes "summary" unserer Daten? Wie aussagekräftig ist der Mittelwert, den ich in den meisten Reports angezeigt bekomme?

Daten sind Vergangenheit, _in the past_. Vergangene Aktienkurse; vergangene Verkaufszahlen; vergangene Adclicks. Interessiert mich der Schnee von gestern? Ja - weil ich auf den Schnee von morgen schliessen möchte! Wie kann ich das machen? Wie kann ich die in den Daten enthaltene Information optimal nutzen und die relevanten Variablen "herausfischen"? Hier ist das Business-Know-How wichtig - aber nicht alles. Methoden aus Data Science, Machine Learning und Statistik warten nur darauf, mich zu unterstützen.

Und schliesslich gibt es eine Klasse von Daten, die anders sind - scheinbar unstrukturiert; vielschichtig; extrem hochauflösend und überwältigend schon aufgrund ihrer Quantität. Für uns Menschen ist es selbstverständlich, dass wir Sprache verstehen, einen Hund von einer Katze unterscheiden können, dass fahrende Autos anders klingen, wenn es nass als wenn es trocken ist. Auf Pixelebene aber gibt es nichts, was einen Hund zum Hund oder eine Katze zur Katze macht. 
Diese Daten, diese Aufgabenstellungen sind das Reich von Deep Learning, das mit gar nicht so neuen Konzepten, aber immer ausgefeilteren Algorithmen und viel Rechenpower jeden Monat neue Rekorde zu brechen scheint. Aber das betrifft doch nur "die Grossen" - Google, Facebook, Microsoft, Apple ... oder? Oder ist Deep Learning auch etwas für mich und meine Daten?

## Pfade durch den Jungle

Wenn heute Daten ein Jungle sind, sind Algorithmen es noch mehr. In diesem Artikel wollen wir Pfade durch den Algorithmenjungle ziehen, die Ihnen helfen sollen, die für Ihre Anforderungen und Bedürfnisse relevanten Methoden zu finden.
Daten werden dabei für uns vor allem in zwei Inkarnationen auftreten: Zum einen als Herausforderung. Hier geht es darum, die übergrosse Datenmenge in den Griff zu bekommen, zu summarisieren, ohne wichtige Features zu übersehen oder zu verfälschen.
Zum anderen als Chance. Welche Methoden stellen Data Science, Machine Learning, Deep Learning uns zur Verfügung, um Muster in den Daten zu finden und valide Schlussfolgerungen zu ziehen? Dabei geht es nicht um ein Potpourri der Methoden, ein Who's who der Algorithmen mit dem Ziel, alles einmal erwähnt zu haben. Es geht um den Überblick: Wie unterscheiden sich die Herangehensweisen, was sind die Konsequenzen, wenn ich mich für Methode B statt Methode A entscheide? Welches Vorgehen passt zu welchen Daten, zu welchen Fragestellungen? Es geht darum, den Jungle zu "mappen", um das geeignete Vorgehen für den konkreten Anwendungsfall finden zu können.

# Teil 1: Daten als Herausforderung

Bevor wir fragen, was Data Science und Artificial Intelligence uns geben können, sollten wir schauen, was wir mitnehmen auf die Reise durch den Dschungel. Das ist v.a. ein Verständnis für die Grundlagen - wie exploriere ich Daten, wie charakterisiere ich sie so präzis wie möglich?


## Was ist ein gutes Summary?

Dass Mittelwerte ein fragwürdiges Summary sein können, hat jeder schon einmal gehört. Aber warum eigentlich ist das so bzw. in welchen Fällen?
Schauen wir uns zunächst einen Fall an, wo es nicht schadet, einen Mittelwert anzugeben: Abb. 1a.

```{r, echo=FALSE}
g1 <- ggplot(algae, aes(x = mxPH)) + geom_density(na.rm = TRUE) + 
  geom_vline(aes(xintercept = mean(algae$mxPH, na.rm = TRUE), linetype = "Mittelwert"), color = 'red') + 
  geom_vline(aes(xintercept = median(algae$mxPH, na.rm = TRUE), linetype = "Median"), color = 'green') + 
  scale_linetype_manual(name = "Mass", values = c(2, 2), 
                      guide = guide_legend(override.aes = list(color = c("green", "red")))) +
 xlab("") + ylab("Dichte") + theme(legend.position="bottom")

g2 <- ggplot(algae, aes(x = a1)) + geom_density(na.rm = TRUE) + 
  geom_vline(aes(xintercept = mean(algae$a1, na.rm = TRUE), linetype = "Mittelwert"), color = 'red') + 
  geom_vline(aes(xintercept = median(algae$a1, na.rm = TRUE), linetype = "Median"), color = 'green') + 
  scale_linetype_manual(name = "Mass", values = c(2, 2), 
  guide = guide_legend(override.aes = list(color = c("green", "red")))) +
  xlab("") + ylab("Dichte") + theme(legend.position="bottom") + ggtitle("Dataset A")

grid.arrange(g1, g2, ncol = 2)
```

Der Mittelwert liegt tatsächlich mittig in der Verteilung, direkt neben dem Median, dem Wert, unterhalb und oberhalb dessen jeweils die Hälfte der Werte liegen. Tatsächlich ist die hier gemessene Variable mehr oder weniger normalverteilt. Eine Normalverteilung ist symmetrisch und durch Mittelwert und Streuung (Varianz) hinreichend charakterisiert.


### Der Median

Wie kann es aber ausschauen, wenn die Daten weniger "schön normalverteilt" sind? Ein Beispiel sehen wir in Abb. 1b. Während der Median das Gros der Daten, das nahe 0 liegt, gut beschreibt, ist der Mittelwert weit nach rechts verschoben. Warum? Beim Median kommt es allein auf die Ordnung der Werte an, die Abstände sind unerheblich. Beim Mittelwert gehen die Abstände direkt in die Berechnung ein. Krasses Beispiel: Der Median der Liste (1,2,3,4,1000) ist 3, der Mittelwert aber 202!

Insofern ist der Median, wenn es um knappe Charakterisierung der Daten geht, schon einmal ein besserer Kandidat als der Mittelwert. Für wirklich wichtige Daten und wenn ich den Platz habe, ersetzt aber nichts die Darstellung der gesamten Verteilung. 

Wie aussagekräftig ist z.B. der Median, wenn ich Daten habe wie in Abb. 2?

```{r, echo=FALSE, warning=FALSE, message=FALSE}
betas <- rbeta(100,.5,.5)
ggplot(data.frame(x=betas), aes(x = x)) + geom_histogram(bins = 20) +
  geom_vline(aes(xintercept = mean(x), linetype = "Mittelwert"), color = 'red') + 
  geom_vline(aes(xintercept = median(x), linetype = "Median"), color = 'green') + 
  scale_linetype_manual(name = "Mass", values = c(2, 2), 
                      guide = guide_legend(override.aes = list(color = c("red", "green")))) +
  ylab("Anzahl") + ggtitle("Dataset B")

  
```


### Boxplots

Eine beliebte klassische Methode, eine ganze Verteilung zu visualisieren, statt sie auf eine einzige Zahl zu reduzieren, ist - seit langem - das Boxplot (Abb. TBD). Für den Fall in Abb. TBD funktioniert das auch ziemlich gut: Wir sehen zum einen den Median (dicke horizontale Linie) und die für den Namen verantwortliche Box. In der Box liegen alle Werte zwischen dem 25. und dem 75. Perzentil. (Kurz zur Terminologie: Das n-te Perzentil, oder auch 0.n Quantil, ist der Wert, unterhalb dessen in der Verteilung n Prozent der Werte liegen. Der Median ist also auch das 50. Perzentil oder 0.5 Quantil. Häufig trifft man auch auf die Bezeichnung Quartil; das 1. Quartil ist dann gleich dem 25. Perzentil, das 3 . Quartil gleich dem 75. Perzentil.) Die vertikalen Linien ("Barthaare") haben mit dem sog. inter quartile range (IQR) zu tun. Der IQR ist der Abstand zwischen dem 1. und dem 3. Quartil. Das obere Barthaar reicht dann bis zu dem grössten Wert, der nicht mehr als 1.5 mal IQR von der oberen Boxgrenze entfernt ist; das untere entsprechend bis zum kleinsten Wert, dessen Entfernung zur unteren Boxgrenze nicht mehr als 1.5 mal IQR beträgt. Werte, die dann noch ausserhalb liegen, sind als Punkte eingezeichnet und werden oft als Outlier gesehen.
Was zeigt uns nun das Boxplot in Abb. TBD? Man sieht unmittelbar, dass das obere Rechteck viel höher ist als das untere: Das heisst, zwischen dem 25. Perzentil und dem Median liegen die Daten sehr eng zusammen; zwischem dem Median und dem 75. Perzentil liegen sie weiter gestreut. Dasselbe Bild zeigen uns die "Barthaare": die obere Linie reicht sehr weit nach oben, während die untere direkt bei 0 trunkiert wird. Nicht zufällig sind, was hier dargestellt wird, dieselben Daten wie in Abb. TBD. Wir können die Aussagekraft des Boxplots noch steigern, indem wir, wie in Abb. TBD geschehen, Mittelwert und Modus (den häufigsten Wert der Verteilung, in diesem Fall 0) hinzufügen.


```{r, echo=FALSE}
g1 <- ggplot(algae, aes(x = 1, y = a1)) + geom_boxplot() + ylab("")
mod <-algae %>% group_by(a1) %>% summarize(n = n()) %>% arrange(desc(n)) %>% select(a1) %>% head(1)
g2 <- ggplot(algae, aes(x = 1, y = a1))  + geom_boxplot() + 
  geom_hline(aes(yintercept = mean(algae$a1)), color = "green", linetype=2) + 
  geom_hline(aes(yintercept = mod), color = "blue", linetype=4)  + 
  ylab("")
grid.arrange(g1, g2, ncol= 2, top = "Dataset A")
```

### Violin plots

Wie hilfreich ist ein Boxplot für die Daten in abb. TBD? Abb. TBD zeigt es: Man sieht nicht ohne weiteres, dass hier eine bimodale Verteilung, mit den Modi an den beiden Enden, vorliegt. Das TBD an der y -Achse, um das wir das Boxplot ergänzt haben, ist da schon hilfreicher. Warum dann aber nicht gleich die ganze Verteilung zeigen, wenn das genauso viel Platz braucht? Abb. TBD zeigt ein sog. violin plot, nichts anderes als eine rotierte und gespiegelte Dichtekurve. Hier sieht man sofort, was los ist!



```{r, echo=FALSE}
g1 <- ggplot(data_frame(y = betas), aes(x = 1, y = betas)) + geom_boxplot() + geom_rug() +
  ylab("")
g2 <- ggplot(data_frame(y = betas), aes(x = 1, y = betas)) + geom_violin() + geom_rug() + ylab("")

grid.arrange(g1, g2, ncol= 2, top = "Dataset B")
```

### Quantile plots

Auf den ersten Blick weniger leicht zu lesen als violin plots, dafür aber sehr aussagekräftig sind quantile plots. Hier werden die Originaldaten dem Wert nach geordnet und dann gegen ihre Position in der Ordnung, ausgedrückt als Proportion zwischen 0 und 1, geplottet.

```{r, echo=FALSE}
sorted <- sort(algae$a1)
df <- data_frame(x = (1:(length(sorted)) - 0.5)/length(sorted), y = sorted)
g1 <- ggplot(df, aes(x=x, y=y)) + geom_point() + scale_x_continuous(c(0.0,1.0)) +
  ggtitle("Dataset A")
sorted <- sort(betas)
df <- data_frame(x = (1:(length(sorted)) - 0.5)/length(sorted), y = sorted)
g2 <- ggplot(df, aes(x=x, y=y)) + geom_point() + scale_x_continuous(c(0.0,1.0)) +
  ggtitle("Dataset B")
grid.arrange(g1, g2, ncol= 2)
```

Von den Quantilplots können wir nicht nur problemlos die wichtigen Quantile (0.25, 0.5, 0.75) ablesen.
Wir haben zudem durch die Steigung ein gutes Gefühl für die Dichte der Daten. Vergleichen wir die beiden Quantilplots in Abb. TBD und Abb. TBD: Links haben wir Dataset A, bei dem die niedrigen Werte eng zusammenliegen. Die sehr flache Steigung links des Medians zeigt, dass die Hälfte der Daten zusammenkommt, ohne dass der Wert merklich steigt. Nach dem Median nimmt die Kurve zunehmend Fahrt auf: Jetzt werden mit hinzukommenden Daten immer grössere Wertabstände überwunden.
Ganz anders Dataset B, das nahezu eine S-Kurve beschreibt. Die Steigung ist maximal in der Gegend um den Median herum. An beiden Enden liegen mehr Werte als in der Mitte, daher die flache Steigung an den Enden des S.

### Informationsdichte und Plot-Design

Womit wir uns hier beschäftigt haben, ist die Aussagekraft und Informationsdichte von Plots und Kennzahlen. Das Wort Informationsdichte kann man aber auch wörtlicher verstehen: Wie lassen sich Plots designen, die den vorhandenen Raum optimal ausnutzen, d.h. bei gegebener räumlicher Beschränkung maximale Information übermitteln?


Leider haben wir hier nicht den Raum, ausführlich auf gestalterische Aspekte der Visualisierung einzugehen. Wer sich hierfür interessiert, dem seien die Bücher von Edward Tufte empfohlen [1]. Ein guter Einstieg ist der Essay über den Cognitive Style of Powerpoint([2]). Wer den zum ersten Mal liest, wird nicht nur seine ggf. präexistenten Bedenken gegen den bullet point style bestätigt finden (von Gewährspersonen wie Richard Feynman!): Das Eindrucksvollste ist, wie absurd die geringe Informationsdichte von typischen Powerpoint-"Tabellen" oder - Diagrammen plötzlich erscheint, wenn man sie mit hoch informativen Grafiken wie sparklines([3]) oder slope graphs([4]) vergleicht.


## Warum ist die Streuung der Daten wichtig?

Zurück zu den Kennzahlen. Bisher haben wir nur über Summaries bestehender Daten gesprochen.
Vielleicht ist das auch genug: Vielleicht möchte ich tatsächlich nur zusammenfassen, wie, unter gegebenen, ganz konkreten Bedingungen, "die Zahlen sich entwickelt haben". Aber oft will ich mehr: in andere Kontexte extrapolieren, zukünftige Werte vorhersagen oder (worum es im zweiten Teil gehen wird!) Zusammenhänge zwischen Variablen finden.

Sobald wir verallgemeinern, vorhersagen oder extrapolieren, verlassen wir das Reich der Gewissheiten. Es ist essentiell, dass wir uns über den Unsicherheitsfaktor im klaren sind und ihn quantifizieren können. Die meisten der Methoden, die wir in Teil zwei besprechen werden, kommen bereits mit Massen für die Unsicherheit. Was kann man machen, wenn die Methode kein solches Mass liefert? 

Eine universell einsetzbare Methode ist der Bootstrap. Das geht so: Ich möchte die Beziehung zwischen zwei Variablen, sagen wir x und y, "allgemeingültig" charakterisieren, habe aber nur ein fixes Set von Daten. Dieses Dataset ist eine mögliche Stichprobe von Werten aus der Population aller Werte. Nun behandle ich einfach mein Dataset, meine Stichprobe, selber als Population: Ich ziehe wiederholt Stichproben aus der Population und berechne jeweils die Beziehung zwischen x und y. (Die Analogie ist cum grano salis zu nehmen; beim Bootstrap handelt es sich um _sampling mit replacement_, d.h. ein und derselbe Wert darf mehrmals gezogen werden, während das "in der Realität" meist nicht der Fall sein dürfte.) Jetzt habe ich also z.B. 100 Schätzungen meines Kennwertes und berechne die Standardabweichung, die durchschnittliche quadrierte Abweichung der Schätzungen von ihrem Mittelwert.

Gut - zum Umgang mit Streuung und Unsicherheit ist also der wichtigste Punkt, dass wir sie angeben müssen.
Aber was exakt soll ich angeben? Vielleicht haben Sie schon einmal die Floskel "+/-2 Standardabweichungen" gehört. Woher kommt das und was bedeutet diese Information?
Grundlage des ganzen ist das Central Limit Theorem. Das Central Limit Theorem besagt, dass Mittelwerte einer Zufallsvariable im Limit unendlicher Stichprobengrösse normalverteilt sind. Die Normalverteilung der Mittelwerte hat als Mittelwert den wahren Mittelwert und als Standardabweichung den sog. Standardfehler des Mittelwerts (die Standardabweichung der gemessenen Variable, geteilt durch die Wurzel der Stichprobengrösse). 
Was bringt uns das? Für die Normalverteilung wissen wir, wieviel Prozent der Verteilung zwischen dem Mittelwert und einer, zwei oder drei Standardabweichungen liegen (Abb. TBD)

```{r, echo=FALSE}
normal_prob_area_plot <- function(lb, ub, mean = 0, sd = 1, limits = c(mean - 4 * sd, mean + 4 * sd), fcolor = "blue", title = "") {
    x <- seq(limits[1], limits[2], length.out = 100)
    xmin <- max(lb, limits[1])
    xmax <- min(ub, limits[2])
    areax <- seq(xmin, xmax, length.out = 100)
    area <- data.frame(x = areax, ymin = 0, ymax = dnorm(areax, mean = mean, sd = sd))
    (ggplot()
     + geom_line(data.frame(x = x, y = dnorm(x, mean = mean, sd = sd)),
                 mapping = aes(x = x, y = y))
     + geom_ribbon(data = area, mapping = aes(x = x, ymin = ymin, ymax = ymax), fill = fcolor)
     + scale_x_continuous(limits = limits) + ggtitle(title))
}
g1 <- normal_prob_area_plot(-3,3, title = "99% Dichte")
g2 <- normal_prob_area_plot(-2,2, title = "95% Dichte")
g3 <- normal_prob_area_plot(-1,1, title = "68% Dichte")
grid.arrange(g3,g2,g1, ncol=2, nrow=3, top="")
```

Damit kann ich für meine Parameterschätzung ein sog. Konfidenzintervall angeben. Leicht vereinfachend ausgedrückt: Wenn ich sage, meine Schätzung für den Parameter ist p +/- 2 mal Standardfehler, dann bin ich mir 95% sicher, dass der Parameter in diesem Bereich liegt. (Das ist in der Tat eine Vereinfachung. Siehe Literatur (TBD).) 
Was ich jetzt noch bedenken muss: Ein Konfidenzintervall für einen Parameter ist nicht dasselbe wie ein sog. Vorhersageintervall (_prediction interval_). Das Vorhersageintervall bezieht sich auf eine konkrete Vorhersage für neue Daten, und ist (z.T. erheblich) breiter als das Konfidenzintervall. Wir werden in Teil 2 ein Beispiel sehen.


## Was charakterisiert eine Beziehung?

"Die Korrelation zwischen Investition in Onlinemarketing und Produktabsatz beträgt 0.8". Ein hervorragendes Ergebnis, oder? (Der Korrelationskoeffizient liegt zwischen 0 und 1; tatsächlich ist 0.8 eine sehr hohe Korrelation.) Nun ja, vielleicht. Auch hier gilt wieder: Ein Bild sagt mehr als 1000 Kennzahlen.
Nehmen wir an, ich habe ein bivariates Dataset mit Variablen x und y. Ich kenne den Mittelwert von x (~54.26), den Mittelwert von y (~47.83) und die jeweiligen Standardabweichungen (~16.76 resp. ~26.93). Vor allem kenne ich die Korrelation zwischen x und y (~-0.06). r = -0.06 bedeutet, die Daten sind unkorreliert, es gibt also keinen interessanten Zusammenhang. Wirklich?
Schauen wir auf Abb. TBD: Alle diese Datasets erfüllen die o.g. Bedingungen ([TBD]). Jede Menge Zusammenhänge - nur eben nicht der lineare, den der Korrelationskoeffizient voraussetzt!

```{r, echo=FALSE}
ggplot(datasaurus_dozen, aes(x=x, y=y, colour=dataset))+
    geom_point()+
    theme_void()+
    theme(legend.position = "none")+
    facet_wrap(~dataset, ncol=3)
```

Berühmt ist auch Simpson's Paradox, bei dem sich die Einschätzung komplett ins Gegenteil verkehrt, je nachdem, ob man die Daten als Gesamtheit betrachtet oder getrennt nach Subgruppen. Siehe Abb. TBD: Die bei Gesamtbetrachtung stark positive Beziehung der Variablen wird zu einer stark negativen, wenn man das Dataset in Gruppen aufteilt.

```{r, echo=FALSE}
ggplot(simpsons_paradox, aes(x=x, y=y, colour=dataset))+
    geom_point()+
    theme_void()+
    theme(legend.position = "none")+
    facet_wrap(~dataset, ncol=3)
```

Wie visualisiere ich nun am besten den Zusammenhang zwischen zwei Variablen? Das klassische Scatterplot ist dazu hervorragend geeignet. Beim normalen Scatterplot ist es allerdings nicht unbedingt einfach, die univariaten Verteilungen zu "sehen". Hier hilft es, z.B. ein Histogramm oder eine Dichtekurve hinzuzufügen, parallel zu den Achsen (Abb. TBD)

```{r, echo=FALSE}
g1 <- ggplot(datasaurus_dozen_wide, aes(x = bullseye_x, y = bullseye_y)) + geom_point() +
  theme_tufte(ticks=F) + theme(axis.title=element_blank(), axis.text=element_blank())
g1 <- g1 %>% ggMarginal(type = "density")
g2 <- ggplot(datasaurus_dozen_wide, aes(x = bullseye_x, y = bullseye_y)) + geom_point() +
  theme_tufte(ticks=F) + theme(axis.title=element_blank(), axis.text=element_blank())
g2 <- g2 %>% ggMarginal(type = "histogram", fill="transparent", bins = 20)
grid.arrange(g2,g1, ncol=2)
```

Diese Form der Visualisierung funktioniert hervorragend für bivariate Daten. Was, wenn die Zahl der interessierenden Varialen weit höher ist als 2?

## Aber die Dimensionalität meiner Daten ist viel zu hoch!

dataset: dmwr / mnist
https://www.analyticsvidhya.com/blog/2017/01/t-sne-implementation-r-python/

```{r, echo=FALSE}
iris_unique <- unique(iris) # Remove duplicates
iris_matrix <- as.matrix(iris_unique[,1:4])
set.seed(42) # Set a seed if you want reproducible results
tsne_out <- Rtsne(iris_matrix) # Run TSNE

# Show the objects in the 2D tsne representation
plot(tsne_out$Y,col=iris_unique$Species)

# Using a dist object
tsne_out <- Rtsne(dist(iris_matrix))
plot(tsne_out$Y,col=iris_unique$Species)
```

# Teil 2: Daten als Chance

Nachdem wir jetzt das nötige Rüstzeug haben, was der Dschungel an Schätzen für uns bereithält.
Was für Erkenntnisse können wir aus den Daten ziehen, was für Methoden setzen wir ein?
Was wir einsetzen, hängt von unserer Fragestellung und der Art der Daten ab.
Haben wir gewohnte, strukturierte Daten wie Verkaufszahlen, Ausgaben, Kundendaten, Business-Metriken o.ä., befinden wir uns im Bereich klassischer Methoden wie Regression, Decision Trees, Clusteranalyse etc. Ob man das eher _Data Science_ nennt, _Machine Learning_, _Data Mining_ oder _Advanced Analytics_, ist eine Frage der Vorlieben, der Richtung, aus der man kommt, oder des Umfelds, in dem man arbeitet - die Algorithmen dahinter sind dieselben. Auch die Übergänge zur Statistik sind fliessend. (Nicht umsonst wird im Literaturteil insbesondere ein Buch mit dem Titel "Introduction to Statistical Learning" empfohlen.) Im folgenden werde ich meist von Data Science sprechen.

Wenn also das (selbst unter der "Hype-Bezeichnung" Data Science!) das klassische Vorgehen ist, was ist dann die Alternative? Das ist das sogenannte _Deep Learning_, mit dem in Bereichen wie Bilderkennung, maschinelle Übersetzung oder Robotik zur Zeit jedes Jahr neue Rekorde gebrochen werden. Auch wenn Deep Learning prinzipiell auf "klassische" Daten anwendbar ist, so liegen seine Stärken doch vor allem in Aufgabenstellungen wie den genannten. Deep Learning stellt die gewohnten Denkweisen auf den Kopf: Es geht nicht darum, das zu lösen, was für uns Menschen schwer und zeitraubend ist (komplizierte Kalkulationen zum Beispiel), sondern das, was uns leicht fällt: sprechen, verstehen, sehen, gehen. Traditionell sind es genau diese Dinge, die für Maschinen schwer sind - das ist es, was sich mit Deep Learning nun zunehmend ändert.
Aber bevor wir uns Deep Learning genauer anschauen, wollen wir sehen: Was geht mit, wozu hilft uns und wie macht man _Data Science_?

## Goodness of fit

Die Diskussion dessen, was wir mit Data Science machen können, wird unter dem Motto _Vorhersagen und Erklären_, oder auch: _Vorhersagen vs. Erklären_, stehen. Um Missverständnisse zu vermeiden, müssen wir eine Sache vorab klären: Wie messe ich, wie gut ein Modell ist?
Das Allerwichtigste: Immer an neuen Daten; niemals an denen, mit denen ich das Modell gefittet habe.
Egal, mit welcher Methode ich arbeite: Als erstes teile ich immer die Daten in zwei, ggf. drei, Sets, ein Training Set, ein Validation Set (optional) und ein Test Set.
Das Training Set benutze ich, um ein Modell zu fitten - sagen wir, ein Modell zur Bestimmung von Kundenzufriedenheit aus Faktoren wie Produktkategorie, Preiskategorie, Schnelligkeit der Lieferung etc. Während des gesamten Model Fittings lasse ich mein Test Set unberührt. Das Test Set verwende ich ausschliesslich, um die Performance des Modells abschliessend zu prüfen.
Gut - viele Modelle aber haben Parameter, die ich variieren kann (und sollte!), um die bestmöglichen Einstellungen zu finden. Wie kann ich wissen, welches Setting schlussendlich das beste Ergebnis liefern wird, wenn ich das Test Set nicht konsultieren darf?
Zu diesem Zweck habe ich möglicherweise das Validation Set. In diesem Fall würde ich das Validation Set benutzen, um die Performance unter verschiedenen Permutationen von Parameter Settings zu testen.
Häufiger ist der Einsatz von Kreuzvalidierung (_cross validation_). Bei der Kreuzvalidierung benutze ich das Training Set, um die besten Settings zu bestimmen. Das geht so: Ich unterteile das Training Set in _n_ Subsets (meist 5 oder 10). Nehmen wir 5 als Beispiel. Jetzt iteriere ich über die Subsets, und zwar so. Im ersten Schritt nehme ich die Subsets 1 bis 4, lege sie zusammen und trainiere auf dieser Datenbasis meine verschiedenen Modelle. (Der Einfachheit halber spreche ich hier von "Modellen", es kann sich aber durchaus um ein und denselben Algorithmus handeln, jeweils mit verschiedenen Parameter Settings.) Die Performance dieser Modelle teste ich nun am fürs Fitten nicht benutzten Subset, Subset 5. Dann wiederhole ich den Vorgang mit Subsets 2-5 fürs Fitten und Subset 1 zum Testen; Subsets 3,4,5,1 fürs Fitten und Subset 2 zum Testen; und so weiter, bis ich jedes Subset einmal zum Testen verwendet habe. Die Ergebnisse für die verschiedenen Modelle werden gemittelt, und ich habe eine valide Entscheidungsgrundlage für die Modellauswahl. Die Performance auf dem Test Set wird dann separat bestimmt.
Wie funktioniert die Trennung in Training Set und Test Set? Im Normalfall einfach als Zufallsauswahl.
Anders verhält es sich bei Zeitreihen, also Daten, die intrinsisch zeitlich geordnet sind. Hier würde man jeweils zusammenhängende Zeiträume für das Training Set und für das Test Set wählen.
Auf die Gefahr, die Wiederholung zu übertreiben: Die Performance eines Modells wird abschliessend immer nur am Test Set evaluiert. Wenn im folgenden, der Kürze halber, zu einer Methode nur ein Dataset gezeigt wird, ist immer das Training Set gemeint, das Set, auf dem wir das Modell fitten. 
Jetzt aber schauen wir erst einmal, was es mit dem Gegensatz _Vorhersagen vs. Erklären_ auf sich hat.
.

## Vorhersagen vs. Erklären

Schauen wir auf Abb. TBD. Links sehen wir ein klassisches Beispiel für sogenannte _Time series_  - (Zeitreihen-) Daten: monatliche Zahlen von Fluggästen. Zeitreihen können auch multivariat sein, d.h. mehrere Variablen umfassen, die man dann aufeinander beziehen kann - häufig aber haben wir eine einzelne Zeitreihe vorliegen und möchten Aussagen über den weiteren Verlauf machen.
Hier ist das Ziel ganz klar: Wir möchten eine möglichst korrekte Vorhersage! Damit haben wir ein ganz einfaches Entscheidungskriterium, welche Methode wir wählen sollen: Wir wählen die, die die beste Vorhersage liefert! (Für die Bestimmung der "besten Vorhersage" gilt, wie gerade besprochen: Model Fitting am Training Set und abschliessende Beurteilung am Test Set.).
Im abgebildeten Beispiel wurde ARIMA verwendet, die wahrscheinlich meistverwendete Methode zur isolierten Betrachtung einzelner Zeitreihen. Selbst wenn man sich am Ende für einen anderen Algorithmus entscheiden sollte, ist ARIMA immer eine gute Wahl für eine exzellente Baseline. Für das Thema Zeitreihen sei nun auf den Literaturteil verwiesen, da wir hier nicht speziell auf dieses komplexe und faszinierende Thema eingehen können.


```{r, echo=FALSE}
g1 <- autoplot(AirPassengers)
fit <- auto.arima(AirPassengers)
g2 <- autoplot(forecast(fit))
grid.arrange(g1,g2, ncol=2)
```


Nehmen wir nun an, wir haben eine Situation wie in A. TBD. Hier geht es darum, den Verkaufspreis eines Anwesens aus Variablen wie Wohnraum, Grösse der Garage etc. vorherzusagen. Wie bei den Zeitreihen kann es auch hier einfach das Ziel sein, die bestmögliche Vorhersage zu machen (in diesem Fall, einen optimalen Verkaufspreis zu bestimmen, der weder potentielle Käufer verschreckt noch zu einem Verkauf unter Wert führt).Ich kann aber auch das Ziel haben, etwas über die Welt herauszufinden: In diesem Fall: Was bestimmt eigentlich den Verkaufspreis? Das mag bei der Beschränkung auf Quadratmeterangaben und dergleichen wenig spannend wirken, wird aber schon anders, wenn wir sozioökonomische, geographische, politische Fakten hinzunehmen. Damit hier keine Missverständnisse aufkommen: Natürlich bedeutet der Fokus auf Erklären/Verstehen keinesfalls, dass wir hier notwendig ein wissenschaftliches Interesse verfolgen. Das kann sein, muss aber nicht. Es kann auch einfach darum gehen, durch Abstraktion die Entscheidungsfindung zu erleichtern. 
Je nachdem, was uns primär interessiert - die Genauigkeit der Vorhersage oder ein erklärendes Modell -, werden wir den Schwerpunkt auf unterschiedliche Methoden legen.


```{r, echo=FALSE, message=FALSE, warning=FALSE}
ames <- read_csv("ames_housing_train.csv")
ames_part <- ames %>% select(SalePrice,GrLivArea,FullBath,TotRmsAbvGrd,GarageYrBlt,GarageCars,GarageArea) %>% filter(GrLivArea < 4000) 
ames_part %>% ggpairs()
```



## Erklären

Wenn es mir primär ums Verstehen geht, sind lineare Modelle optimal - vorausgesetzt, die Daten lassen sich mit einem linearen Modell hinreichend gut abbilden. Warum? Wenn ich einen linearen Zusammenhang habe, kann ich die Schlussfolgerung ziehen: Erhöhung von x um eine Einheit führt zu Erhöhung von y um eine Einheit. Schauen wir uns das in einem Beispiel an (Abb. TBD):


```{r, echo=FALSE, message=FALSE, warning=FALSE}
fit1 <- lm(SalePrice ~ GrLivArea, data = ames_part)
g1 <- ggplot(ames, aes(x = GrLivArea, y = SalePrice)) + geom_point() + 
  geom_abline(intercept = fit1$coefficients[1], slope = fit1$coefficients[2]) + 
  ggtitle(paste0("R²: ", round(summary(fit1)$adj.r.squared,3), ", beta: ", round(fit1$coefficients[2],2)))
fit2 <- lm(SalePrice ~ I(GrLivArea^2), data = ames_part)
g2 <- ggplot(ames, aes(x = GrLivArea, y = SalePrice)) + geom_point() + 
  stat_function(fun = function(x) fit2$coefficients[1] + fit2$coefficients[2] * x^2) +
  ggtitle(paste0("R²: ", round(summary(fit2)$adj.r.squared,3), ", beta: ", round(fit2$coefficients[2],2)))
grid.arrange(g1,g2, ncol=2)

```

Die Gerade zeigt, was für ein Modell gefittet wurde: Mit jedem zusätzlichen Quadratfuss Wohnfläche steigt der Verkaufspreis in der Stichprobe um etwa 107 Dollar (beta in der Überschrift der Grafik). Das R² im Titel sagt uns, wie gut das Modell auf diese Daten (Trainingsdaten! s.o.) passt: Hier wurde 50% der Varianz in den Verkaufspreisen durch die Varianz in der Wohnfläche erklärt.

Wenn wir auf Abb TBD schauen, können wir uns fragen, ob hier eine Gerade wirklich am besten die Daten beschreibt. Es sieht aus, als müsste eine Parabel besser passen, oder? Wir können, ohne das lineare Modell zu verlassen, eine Parabel fitten, indem wir die Prädiktor-Variable Wohnfläche quadrieren, wie in Abb TBD geschehen. Optisch sieht das besser aus, der _goodness of fit_ - Wert R² ist aber tatsächlich niedriger.

Wie erklärend ist das Modell in Abb.TBD? Hier steigt der Verkaufspreis mit dem Quadrat der Wohnfläche. Das ist immer noch eine stark abstrahierende Aussage. Wenn wir Abb TBD und TBD vergleichen, können wir auf den Gedanken kommen, dass im unteren Range der Wohnflächenwerte der lineare Fit besser ist, im oberen der quadratische. Wir können zwei getrennte Regressionen rechnen, und kämen zu einem Ergebnis der Art: "Für niedrigere Wohnflächen ist der Zusammenhang mit dem Verkaufspreis eher linear, für grössere eher quadratisch". Vielleicht gehen wir aber noch weiter, und zerlegen den Wertebereich des Prädiktors in 5, 10, 20 Teile? In wieviele? Hier lassen wir uns vom _goodness of fit_ leiten, wir wählen das Modell, das in der Kreuzvalidierung die beste Vorhersage liefert.


## Vorhersagen


```{r, echo=FALSE, message=FALSE, warning=FALSE}
fit1 <- lm(SalePrice ~ bs(GrLivArea, knots = c(20000)), data = ames_part)
preds1 <- predict(fit1, se = T)
fit2 <- lm(SalePrice ~ bs(GrLivArea, df = 15), data = ames_part)
preds2 <- predict(fit2, se = T)
fit3 <- lm(SalePrice ~ bs(GrLivArea, df = 30), data = ames_part)
preds3 <- predict(fit3, se = T)
ames_part_pred <- ames_part %>% mutate(pred1 = preds1$fit,
                                       upper1 = preds1$fit + 2* preds1$se,
                                       lower1 = preds1$fit - 2* preds1$se,
                                       pred2 = preds2$fit,
                                       upper2 = preds2$fit + 2* preds2$se,
                                       lower2 = preds2$fit - 2* preds2$se,
                                       pred3 = preds3$fit,
                                       upper3 = preds3$fit + 2* preds3$se,
                                       lower3 = preds3$fit - 2* preds3$se
                                       )
g1 <- ggplot(ames_part_pred, aes(x = GrLivArea, y = SalePrice)) + geom_point() +
  geom_line(aes(y = pred1)) + 
  geom_ribbon(aes(ymin=lower1, ymax=upper1), fill="green", alpha=0.2) +
  ggtitle(paste0("R²: ", round(summary(fit1)$r.squared,3)))
g2 <- ggplot(ames_part_pred, aes(x = GrLivArea, y = SalePrice)) + geom_point() +
  geom_line(aes(y = pred2)) + 
  geom_ribbon(aes(ymin=lower2, ymax=upper2), fill="blue", alpha=0.2) +
  ggtitle(paste0("R²: ", round(summary(fit2)$r.squared,3)))
g3 <- ggplot(ames_part_pred, aes(x = GrLivArea, y = SalePrice)) + geom_point() +
  geom_line(aes(y = pred3)) + 
  geom_ribbon(aes(ymin=lower3, ymax=upper3), fill="red", alpha=0.2) +
  ggtitle(paste0("R²: ", round(summary(fit3)$r.squared,3)))
grid.arrange(g1,g2,g3, ncol=3)
```

tbd 2. beispiel wo nichtlinear mehr bringt!!




## Vorhersagen vs. Erklären, multidimensional

Wir haben 

mutlidim lin reg
random forests

## Heute noch Magic, morgen Mainstream: Deep Learning

Deep Learning


## Literatur
Visualisierung: Chambers, Tufte 
https://www.inf.ed.ac.uk/teaching/courses/pi/2016_2017/phil/tufte-powerpoint.pdf

http://charliepark.org/slopegraphs/

https://www.edwardtufte.com/bboard/q-and-a-fetch-msg?msg_id=0001OR&topic_id=1


New Statistics, Lakens


https://github.com/stephlocke/datasauRus/
https://www.autodeskresearch.com/publications/samestats

